import sys
from pickle import dump,load
from copy import copy,deepcopy
from time import time,sleep
from random import shuffle,randint

from BlueNet.Network import Net
from BlueNet.Layer import *
from BlueNet.Activation import GELU
from BlueNet.Optimizer import Adam
from BlueNet.Functions import RMS
from BlueNet.setting import _np,device

import numpy as np
from numpy.random import choice
from numpy import log,sqrt
from pandas import Series

from minishogi import *
from config import *

from numpy.random import rand as rn
from multiprocessing import freeze_support,Pool 


class AlphaZeroNode:
	def __init__(self, state, p, parent=None):
		self.state = copy(state)
		#獲取現在資訊
		self.is_over, self.winner, self.untried = state.situation()
		#千日手偵測
		if self.state.Board.get_repeat()>1:
			self.is_over = True
		
		self.is_expand = False
		self.parent = parent
		self.children = {}
		
		# Generated By network:
		# Q : state value
		# if this state is the end state: set 1 if operater win else -1
		# P : the Probability of choosing this node
		
		# Get by real situation:
		# N : How many times that this node has been visted
		self.P = p
		self.Q = 0
		self.N = 0
		if self.is_over:
			self.update_Q((0,1,-1)[self.state.Board.is_end()])
	
	def weight(self, c_param=1):
		#UCB公式
		return -self.Q + (self.P*c_param*sqrt(self.parent.N)/(1+self.N))
		
	def select(self, c_param=1, noise=False):
		#獲取各節點weight值之後以pandas的series函式找出action
		if noise:
			weights = [float(i.weight(c_param))+rn() for i in self.children.values()]
		else:
			weights = [float(i.weight(c_param)) for i in self.children.values()]
		action = Series(data=weights, index=self.children.keys()).idxmax()
		
		return action, self.children[action]
	
	def expand(self,prob_list):
		#確保各行動之機率值合為1
		prob_list = prob_list/sum(prob_list)
		
		#Expand
		for action in self.untried:
			state = self.state.get_next(action)
			child_node = AlphaZeroNode(state, prob_list[action], self)
			self.children[action] = child_node
		
		#set the status
		self.untried = []
		self.is_expand = True

	def update_Q(self, Q):
		if self.state.pl:		#先手直接增加
			self.Q = (self.Q*self.N + Q)/(self.N+1.0)
		else:					#後手乘上負號(Q為-1~1 負數代表後手優勢)
			self.Q = (self.Q*self.N - Q)/(self.N+1.0)
		self.N += 1
		
		if self.parent:
			self.parent.update_Q(Q)
	
	def get_real_data(self):
		##return (state, real_P)
		real_P = [0 for i in range(self.state.Board.complexity)]
		for action, node in self.children.items():
			real_P[action] = node.N/self.N
		
		return self.state.encode(), real_P


class AoiZero_Net:
	def __init__(self, Conv_block, Policy_block, Value_Block):
		self.C = Conv_block
		self.P = Policy_block
		self.V = Value_Block
	
	#運算函式
	def process(self, state):
		#獲取data
		data = np.asarray([state.encode()],dtype=np.float32)
		
		#先運算res_block的輸出再分給policy_net跟value_net
		conv_out = self.C.process(data)
		policy_out = self.P.process(conv_out)
		value_out = self.V.process(conv_out)
		
		return float(value_out[0]), policy_out[0]
	
	#訓練函式
	def train(self, x, v_t, p_t):
		#轉換型態
		x = _np.asarray(x)
		V_t = _np.asarray(v_t)
		P_t = _np.asarray(p_t)
		
		#先計算輸出
		conv_out = self.C.forward(x)
		V_out = self.V.forward(conv_out)
		P_out = self.P.forward(conv_out,P_t)
		
		#將誤差傳回進行運算
		dV = self.V.back_train((V_out-V_t)/V_t.shape[0])
		dP = self.P.back_train((P_out-P_t)/P_t.shape[0])
		self.C.back_train((dV+dP))
		
		#輸出誤差函數值
		return RMS(V_out,V_t), P_out


class AoiZero:
	def __init__(self, net):
		self.root = None
		self.net = net
		
	def simulation(self, count=50):
		for _ in range(count):
			T1 = time()
			T2 = T3 = 0
			cur_node = self.root		#從目前節點往下擴展
			while True:
				#遇到已結束節點即跳出（並更新N)
				if cur_node.is_over:
					v = (0,1,-1)[cur_node.state.Board.is_end()]
					cur_node.update_Q(v)
					break
				
				if cur_node.is_expand:	#已擴展節點就使用UCB公式選擇下一節點
					_, cur_node = cur_node.select()
				else:					#未擴展節點從網路獲得值後一次擴展
					T2 = time()
					Q_value, Prob_list = self.net.process(cur_node.state)
					cur_node.update_Q(Q_value)
					cur_node.expand(Prob_list)
					T3 = time()
					break
			T4 = time()
			#print(str((T4-T1-T3+T2))[:5], str(T3-T2)[:5], end='\r')
	
	def take_action(self, cur_state, s=500, c=0, noise=False):
		self.root = AlphaZeroNode(cur_state, None)
		if self.root.is_over:			#如果已經結束回傳None
			return None, None, None
			
		self.simulation(s)				#模擬(Alpha zero為從網路獲取值)
		action, next_node = self.root.select(c, noise=noise)
		return action, self.root.get_real_data(), self.root.Q
	
	def train(self,data,epoch=10,batch=100):
		print(len(data))
		
		x = np.array([i[0] for i in data])
		v_t = np.array([i[1] for i in data])
		p_t = np.array([i[2] for i in data])
		
		iter_num = max(len(data)//batch,1)
		batch = min(batch, len(data))
		
		print('=============================')
		for i in range(epoch):
			all_p_loss = []
			all_v_loss = []
			for j in range(iter_num):
				batch_mask = np.random.choice(len(data), batch)
				
				x_batch = x[batch_mask]
				v_batch = v_t[batch_mask]
				p_batch = p_t[batch_mask]
				
				Vloss,Ploss = self.net.train(x_batch,v_batch,p_batch)
				all_v_loss.append(Vloss)
				all_p_loss.append(Ploss)
				print('Epoch{:<3} VLoss:{:<5} PLoss:{:<5}'\
						.format(i+1,str(Vloss)[:5],str(Ploss)[:5]),end='\r')
			
			Vloss_a = sum(all_v_loss)/iter_num
			Ploss_a = sum(all_p_loss)/iter_num
			print('Epoch{:<3} VLoss:{:<20}'.format(i+1,str(Vloss_a)[:5]))
			print('          PLoss:{:<5}'.format(str(Ploss_a)[:5]))
			print('=============================')
		

def AoiZero_selfplay(player,s=50,pl=1):
	a = miniShogi(deepcopy(initial_board))
	moves = 0
	
	train_data = []
	while moves<1000:
		#print(a)
		S = time()
		
		if not moves:
			move, data, value = player.take_action(State(a,pl),s,1.414,True)
		elif moves<6:			#確保有不同的開局(前四手以UCB找下法)
			move, data, value = player.take_action(State(a,pl),s,1.414)
		else:
			move, data, value = player.take_action(State(a,pl),s,0.0)
		E = time()
		print("Moves:{:<4} Cost :{:<5}".format(moves+1,str(E-S)[:5]),end = '\r')
		
		if move==None:		#無路可走
			break
			
		train_data.append(data)
		#a.print_step(move, pl)
		a.move(move, pl)
		
		pl = 1-pl
		moves += 1
	
	#print(a)
	winner = a.is_end()
	v = (0,1,-1)[winner]
	print('{:<25}'.format(('平手','先手獲勝','後手獲勝')[winner]))
	return [[i[0],[v],i[1]] for i in train_data]

def AoiZero_check(players, s=500, pl=1):
	player, best_player = players
	a = miniShogi(deepcopy(initial_board))
	moves = 0
	while True:
		#print(a)
		S = time()
		
		if moves%2 == pl:
			move, data, value = player.take_action(State(a,pl),s,0.0)
		else:
			move, data, value = best_player.take_action(State(a,pl),s,0.0)
		E = time()
		print("Moves:{:<4} Cost :{:<5}".format(moves+1,str(E-S)[:5]),end = '\r')
		
		if move==None:		#無路可走
			break
		
		a.move(move, pl)
		pl = 1-pl
		moves += 1
	winner = a.is_end()
	
	if pl:
		v = (0,-1,1)[winner]
	else:
		v = (0,1,-1)[winner]
	print('{:<25}'.format(('平手','先手獲勝','後手獲勝')[winner]))
	return v
	

if __name__=='__main__':
	freeze_support()
	pool = Pool(8)
	
	try:
		with open('./Aoi55.azero', 'rb') as f:
			net = load(f)
	except FileNotFoundError:
		net = AoiZero_Net(conv_net, policy_net, value_net)
	
	player = AoiZero(net)
	best_player = copy(player)
	for i in range(100):
		print(f"第{i+1}組自我對局")
		
		data = sum(pool.map(AoiZero_selfplay,[copy(player) for i in range(50)]),[])
		
		with open('./data.pylist','wb') as f:
			dump(data,f)
		
		player.train(data,10,50)					#訓練
		
		#跟當前最佳模型對比
		score = sum(pool.map(AoiZero_check,[(copy(player),copy(best_player)) for i in range(10)]))
		if score>0:
			with open('./Aoi55.azero','wb') as f:
				dump(player.net,f)
			
			best_player=copy(player)
